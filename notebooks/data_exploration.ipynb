{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774dbc98-02ce-4435-8695-0466fb54620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "import pypandoc\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a1f72-1192-4f66-b908-566e841c0df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_competence_profile(file_path: Path) -> str:\n",
    "    with file_path.open('r') as file_handle:\n",
    "        return file_handle.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be88611-1d6b-4923-8171-bd3f9881107a",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Download the competence profiles and peer information\n",
    "- Download and extract the folder of competence profiles from gDrive, rename it to \"competence_profiles\" and put it at a place of your choice.\n",
    "- Adjust `COMPETENCE_PROFILES_DICT_PATH` to point to this folder.\n",
    "- Go to prisma, open F12, hit `peers`, open the `graphql` response and paste the json into a file in this folder and give it the name `prisma_peers_graphql.json` as the constant `PRISMA_PEERS_JSON_RESPONSE_FILE_PATH` below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a6188d-b12b-4f4f-bbef-9b36dde2fc5f",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d988ba-604f-414e-9072-4a8e7229cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPETENCE_PROFILES_DICT_PATH = Path('/Users/matt/competence_profiles')\n",
    "PRISMA_PEERS_JSON_RESPONSE_FILE_PATH = Path('prisma_peers_graphql.json')\n",
    "HARCODED_FILTER_WORDS = ['architektur', 'technologien', 'f', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd53d63-8299-4c47-b1b0-4c8f9e612237",
   "metadata": {},
   "source": [
    "# Data Extraction and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841952c-8922-4e17-a2b6-096847a03df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_short_names_of_files(path_to_folder: Path) -> list[str]:\n",
    "    \"\"\"Given all the competence profile files at the path_to_folder location, extract all ipt short names from it.\"\"\"\n",
    "    file_paths = path_to_folder.glob('Competence Profile_???.docx')\n",
    "    \n",
    "    def extract_shortname_from_file_name(file_name: str) -> str:\n",
    "        return file_name.split('_')[1].split('.')[0]\n",
    "    \n",
    "    short_names = [extract_shortname_from_file_name(path.name) for path in file_paths]\n",
    "    print(f'Found {len(short_names)} competence profiles under {path_to_folder}')\n",
    "    return list(short_names)\n",
    "\n",
    "def convert_competence_profile_to_plain_text(competence_profile_file_path: Path) -> str:\n",
    "    \"\"\"This will convert a .docx competence profile to simple text and removes weird characters originating from tables.\"\"\"\n",
    "    \n",
    "    txt = pypandoc.convert_file(str(competence_profile_file_path), 'org')\n",
    "    txt = txt.split('\\n', 1)[1]  # remove first line (comes from profile image)\n",
    "    \n",
    "    # The following filters are necessary since there are a lot of weird symbols due to the tables in the word document in the text.\n",
    "    txt = txt.replace('|', ' ')\n",
    "    txt = txt.replace('-', ' ')\n",
    "    txt = txt.replace('=', ' ')\n",
    "    txt = txt.replace('+', ' ')\n",
    "    txt = txt.replace('*', ' ')\n",
    "    txt = txt.replace('/', ' ')\n",
    "    txt = txt.replace('# begin_quote', ' ')\n",
    "    txt = txt.replace('# end_quote', ' ')\n",
    "    txt = txt.replace('\\n', ' ')\n",
    "    txt = txt.strip()\n",
    "    return txt\n",
    "    \n",
    "def extract_all_words(competence_profile_plain_txt: str) -> list[str]:\n",
    "    \"\"\"\"Given plain text, extract all the words contained in this unstructured text.\"\"\"\n",
    "    \n",
    "    regex_pattern = r\"\\(\\d\\s+('?\\w+)\"\n",
    "    return re.findall(\"[a-zA-ZöäüÖÜÄ\\-\\.'/]+\", competence_profile_plain_txt)\n",
    "\n",
    "def extract_core_competence_words(competence_profile_words: list[str]) -> list[str]:\n",
    "    \"\"\"This will filter the input words such that only words after 'Kernkompetenzen' and before 'ipt Projekte' will remain.\"\"\"\n",
    "    \n",
    "    arrived_at_core_competences = False\n",
    "    core_competence_words = []\n",
    "    for idx, word in enumerate(competence_profile_words):\n",
    "        if word == 'Kernkompetenzen':\n",
    "            arrived_at_core_competences = True\n",
    "            continue\n",
    "        if arrived_at_core_competences:\n",
    "            if word == 'ipt' and competence_profile_words[idx + 1] == 'Projekte':\n",
    "                break\n",
    "            else:\n",
    "                core_competence_words.append(word)\n",
    "    return core_competence_words\n",
    "\n",
    "def to_lower(words: list[str]) -> list[str]:\n",
    "    return [word.lower() for word in words]\n",
    "\n",
    "def filter_out_stop_words(words: list[str]) -> list[str]:\n",
    "    sp = spacy.load('de_core_news_sm')\n",
    "    stop_words = sp.Defaults.stop_words\n",
    "    return [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "def build_competence_dict(base_path: Path) -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Given a base_path pointing to a folder were all the .docx competence profiles reside, build a a competence_dict.\n",
    "    \n",
    "    The competence_dict's structure is as follows\n",
    "    {'ABC': ['masta_skill', 'epic_skill'], ...}\n",
    "    \"\"\"\n",
    "    short_names = get_all_short_names_of_files(BASE_PATH)\n",
    "    competence_dict = {short_name: [] for short_name in short_names}\n",
    "\n",
    "    for idx, short_name in tqdm(enumerate(short_names), total=len(short_names)):\n",
    "        file_path = BASE_PATH / f'Competence Profile_{short_name}.docx'\n",
    "        txt = convert_competence_profile_to_plain_text(file_path)\n",
    "        words = extract_all_words(txt)\n",
    "        core_competence_words = extract_core_competence_words(words)\n",
    "        core_competence_words = filter_out_stop_words(core_competence_words)\n",
    "        core_competence_words = to_lower(core_competence_words)\n",
    "        core_competence_words = [word for word in core_competence_words if word not in HARCODED_FILTER_WORDS]\n",
    "        competence_dict[short_name] = core_competence_words\n",
    "    \n",
    "    return competence_dict\n",
    "\n",
    "def load_peers_from_prisma_response(prisma_response_json_file: Path) -> list[dict]:\n",
    "    with open(prisma_response_json_file) as infile:\n",
    "        json_response = json.loads(infile.read())\n",
    "        user_info_list = []\n",
    "        for user in json_response['data']['listUsers']:\n",
    "            user_info = {\n",
    "                'firstname': user.get('firstname', ''),\n",
    "                'lastname': user.get('lastname', ''),\n",
    "                'abbreviation': user.get('abbreviation', ''),\n",
    "                'picture': user.get('picture', '')\n",
    "            }\n",
    "            if user_info['firstname'] == None:\n",
    "                continue\n",
    "            user_info_list.append(user_info)\n",
    "        return user_info_list\n",
    "    \n",
    "def enrich_prisma_data_with_seniority_and_start_date(user_info_list: list[dict]) -> list[dict]:\n",
    "    base_url = 'https://ipt.ch/de/team/mitarbeiter'\n",
    "    for iptler in tqdm(user_info_list):\n",
    "        firstname = iptler[\"firstname\"].strip().lower().replace('ä', 'a').replace('ö', 'o').replace('ü', 'u')\n",
    "        lastname = iptler[\"lastname\"].strip().lower().replace('ä', 'a').replace('ö', 'o').replace('ü', 'u')\n",
    "        full_url = f'{base_url}/{firstname}-{lastname}'\n",
    "        response = requests.get(full_url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # parse seniority\n",
    "            seniority = soup.find(\"p\", class_=\"team-detail--role\")\n",
    "            iptler['seniority'] = seniority.text.strip().replace('-', ' ')\n",
    "            \n",
    "            # parse starting year\n",
    "            table_heading = soup.find(\"th\", class_=\"team-detail--table-heading\", text=\"Start bei ipt\")\n",
    "\n",
    "            if table_heading:\n",
    "                # Navigate to the corresponding <td> tag\n",
    "                table_data = table_heading.find_next_sibling(\"td\", class_=\"team-detail--table-field\")\n",
    "                if table_data:\n",
    "                    # Extract the text content of the table data\n",
    "                    start_year = table_data.text.strip()\n",
    "                    iptler['start_year'] = start_year\n",
    "    return user_info_list\n",
    "\n",
    "def find_iptlers_with_skills(iptlers: list[dict], competence_dicts: dict[list], skills: list, min_seniority: str = None, print_out: bool = False) -> list[dict]:\n",
    "    seniority_order = {\n",
    "        0: ['Junior Software Engineer'],\n",
    "        1: ['Consultant'],\n",
    "        2: ['Senior Consultant'],\n",
    "        3: ['IT Architect', 'Lead Consultant'],\n",
    "        4: ['Principal Architect', 'Principal Consultant'],\n",
    "        5: ['Principal Architect, Director', 'Principal Consultant, Director'],\n",
    "        6: ['Associate Partner'],\n",
    "        7: ['Partner']\n",
    "    }\n",
    "    all_seniority_levels = sum(seniority_order.values(), [])\n",
    "\n",
    "    \n",
    "    def get_seniority_level(job_title: str) -> int:\n",
    "        for level, titles in seniority_order.items():\n",
    "            if job_title in titles:\n",
    "                return level\n",
    "        raise ValueError(f'Seniority level {job_title} invalid.')\n",
    "    \n",
    "    \n",
    "    if min_seniority is not None and min_seniority not in all_seniority_levels:\n",
    "        raise ValueError(f'min_seniority must be one of {all_seniority_levels}')\n",
    "    \n",
    "    matches = []\n",
    "    for iptler in iptlers:\n",
    "        if min_seniority is not None:\n",
    "            if 'seniority' in iptler.keys():  # only have it if on website\n",
    "                if iptler['seniority'] not in all_seniority_levels:\n",
    "                    continue # e.g. Marketing\n",
    "                if get_seniority_level(min_seniority) > get_seniority_level(iptler['seniority']):\n",
    "                    continue\n",
    "        \n",
    "        matching_skills = []\n",
    "        if iptler['abbreviation'].upper() not in competence_dict.keys():\n",
    "            continue\n",
    "        for skill in skills:\n",
    "            if skill in competence_dicts[iptler['abbreviation'].upper()]:\n",
    "                matching_skills.append(skill)\n",
    "        if len(matching_skills) > 0:\n",
    "            iptler['skill_matches'] = matching_skills\n",
    "            matches.append(iptler)\n",
    "        matches.sort(key=lambda match: len(match['skill_matches']), reverse=True)\n",
    "    if print_out:\n",
    "        seniority_statement = '' if min_seniority is None else f' and seniority above (including) {min_seniority}'\n",
    "        print(f'Found {len(matches)} IPTlers with matching skills{seniority_statement}:')\n",
    "        print(10 * '-')\n",
    "        \n",
    "        for match in matches:\n",
    "            for key, value in match.items():\n",
    "                if key == 'picture': continue\n",
    "                if key == 'skill_matches':\n",
    "                    value = 'Skill matches: ' + f' '.join(value)\n",
    "                if key == 'abbreviation':\n",
    "                    value = value.upper()\n",
    "                if key == 'start_year':\n",
    "                    value = f'Start IPT: {value}'\n",
    "                print(f'{value}')\n",
    "            print(5 * '-')\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75a413-7c35-4686-87d6-0010902471ce",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06967d5c-36f2-40a0-8488-1c70c7146168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud_from_competence_dict(competence_dict: dict) -> None:\n",
    "    \"\"\"Given the competence_dict this will generate and store a word cloud with word sizes weighted on occurrence.\"\"\"\n",
    "    \n",
    "      # These sort of clutter everythin\n",
    "    all_words = list(itertools.chain(*competence_dict.values())) \n",
    "    \n",
    "    all_words = [word for word in all_words if word not in HARCODED_FILTER_WORDS]\n",
    "    wordcloud = WordCloud(width = 1600, height = 1000,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(' '.join(all_words))\n",
    " \n",
    "    # plot the WordCloud image                      \n",
    "    plt.figure(figsize = (15, 15), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.savefig('ipt_competence_word_cloud.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_bar_plot_most_common_skills(competence_dict: dict) -> None:\n",
    "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "    # Initialize the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(6, 12))\n",
    "\n",
    "    # Load the example car crash dataset\n",
    "    counter = Counter(list(itertools.chain(*competence_dict.values())))\n",
    "    skill_df = pd.DataFrame(counter.most_common(), columns=['skill', 'occurrences'])\n",
    "    skill_df = skill_df.loc[skill_df['occurrences'] > 75]\n",
    "    skill_df['normed_occurrence'] = skill_df['occurrences'] / max(skill_df['occurrences'])\n",
    "    \n",
    "    pal = sns.color_palette(\"Greens_d\", len(skill_df))\n",
    "    sns.barplot(x=\"normed_occurrence\", \n",
    "                y=\"skill\", \n",
    "                data=skill_df,\n",
    "                orient = \"h\")\n",
    "    ax.set(xlabel='Normierte Häufigkeit', ylabel='Skill')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    plt.savefig('ipt_skill_occurrence_distribution.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c349b35-3195-4928-87c7-2cbbeb551890",
   "metadata": {},
   "source": [
    "# Executable Code Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd821d-f20a-4bb4-b03a-9e564bf769ca",
   "metadata": {},
   "source": [
    "The following section needs (MacOS) `brew install pandoc` and from within your virtual environment `python -m spacy download de_core_news_sm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf37126-928c-46ba-95e7-61d8d5281411",
   "metadata": {},
   "outputs": [],
   "source": [
    "competence_dict = build_competence_dict(COMPETENCE_PROFILES_DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6787f-db00-429d-b967-765dfae28503",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_from_competence_dict(competence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9a550-b7a5-41cc-809f-df30aaa5dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_bar_plot_most_common_skills(competence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c1a865-cc32-4c0e-9a71-78c380fee573",
   "metadata": {},
   "outputs": [],
   "source": [
    "iptlers = load_peers_from_prisma_response('prisma_peers_graphql.json')\n",
    "iptlers = enrich_prisma_data_with_seniority_and_start_date(iptlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21afa14f-6448-442c-9d20-170fbc043ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = ['kubernetes', 'openshift']\n",
    "\n",
    "matches = find_iptlers_with_skills(iptlers, competence_dict, skills, min_seniority='Consultant', print_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632bfd0-eed3-41c7-baf9-a18e0967f41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
